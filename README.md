# Natural Language Processing (NLP) : Multi-Class Text Classification Project

Implementing the BERT model for a multi-class text classification project. 

Dataset: E-mail data (documents).

BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google. 

Project implemented during my Artificial Intelligence Internship.

## Technologies
This project is created with:
- Python 3.9.6
- Pytorch 1.12.0
- Pytorch Lightning 1.7.7
- Pandas 1.5.1
- Numpy 1.23.4
- Seaborn 0.12.1
- Wandb 0.13.5
